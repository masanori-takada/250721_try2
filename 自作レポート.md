# 自作LLM開発レポート

## どんなLLMを作ったか
36億個の「知識の粒」を持つ日本語専用AI（rinna/japanese-gpt-neox-3.6b）に、
新しい対話能力を教えました。まるで賢い学生に追加の専門知識を覚えさせるような感じです。

## 使った技術
**LoRA（ファインチューニング技術）**: AIの脳全体を書き換えず、「対話専用の小さな脳」を追加する方法。例えるなら、本体のOSはそのままで、新しいアプリをインストールする感覚です。全体の0.09%だけ変更して大きな効果を得られます。

## 学習環境
- **学習データ**: 15,015組の日本語会話例（質問・回答集のようなもの：kunishou/databricks-dolly-15k-ja）
- **使用GPU**: リモートSSHでクラウドGPU使用。最高性能のNVIDIA A100。
- **学習時間**: 10分弱

## 成果
- 日本語で実用的な会話が可能。
- 専門的な会話は発展途上であり、専門的な会話に特化したデータセットによるファインチューニングに課題ありと考察。
- （youtubeの動画参照：https://www.youtube.com/watch?v=BfiBKHf8KOA）

## 特徴
巨大なAI全体を変更せず、小さな「対話専用パーツ」だけ作成。これにより短時間・低コストで実用的な日本語対話AIが完成しました。
