#!/usr/bin/env python3
"""
Rinna-3.6B LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè£… - A100æœ€é©åŒ–ç‰ˆ
å‚è€ƒ: https://note.com/npaka/n/nc387b639e50e
"""

import os
import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    BitsAndBytesConfig
)
from peft import (
    LoraConfig,
    get_peft_model,
    TaskType
)

# åŸºæœ¬ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
model_name = "rinna/japanese-gpt-neox-3.6b"
dataset = "kunishou/databricks-dolly-15k-ja"
peft_name = "lora-rinna-3.6b-optimized"
output_dir = "lora-rinna-3.6b-results-optimized"

CUTOFF_LEN = 256  # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·

def setup_environment():
    """ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
    print("=== A100æœ€é©åŒ–ç‰ˆç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— ===")
    
    # ä½œæ¥­ãƒ•ã‚©ãƒ«ãƒ€ã®ä½œæˆ
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(peft_name, exist_ok=True)
    
    print(f"ãƒ¢ãƒ‡ãƒ«å: {model_name}")
    print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {dataset}")
    print(f"PEFTãƒ•ã‚©ãƒ«ãƒ€: {peft_name}")
    print(f"å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€: {output_dir}")

def prepare_tokenizer():
    """ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®æº–å‚™"""
    print("\n=== ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®æº–å‚™ ===")
    
    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
    
    # ã‚¹ãƒšã‚·ãƒ£ãƒ«ãƒˆãƒ¼ã‚¯ãƒ³ã®ç¢ºèª
    print("ã‚¹ãƒšã‚·ãƒ£ãƒ«ãƒˆãƒ¼ã‚¯ãƒ³:")
    print(f"bos_token : {tokenizer.bos_token} , {tokenizer.bos_token_id}")
    print(f"eos_token : {tokenizer.eos_token} , {tokenizer.eos_token_id}")
    print(f"pad_token : {tokenizer.pad_token} , {tokenizer.pad_token_id}")
    
    return tokenizer

def tokenize(prompt, tokenizer):
    """ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºé–¢æ•°"""
    result = tokenizer(
        prompt,
        truncation=True,
        max_length=CUTOFF_LEN,
        padding=False,
    )
    return {
        "input_ids": result["input_ids"],
        "attention_mask": result["attention_mask"],
    }

def generate_prompt(data_point):
    """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®æº–å‚™"""
    if data_point["input"]:
        result = f"""### æŒ‡ç¤º:
{data_point["instruction"]}

### å…¥åŠ›:
{data_point["input"]}

### å›ç­”:
{data_point["output"]}"""
    else:
        result = f"""### æŒ‡ç¤º:
{data_point["instruction"]}

### å›ç­”:
{data_point["output"]}"""
    
    # æ”¹è¡Œâ†’<NL>
    result = result.replace('\n', '<NL>')
    return result

def prepare_dataset(tokenizer):
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™"""
    print("\n=== ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™ ===")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿
    data = load_dataset(dataset)
    print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚º: {len(data['train'])}")
    
    def generate_and_tokenize_prompt(data_point):
        full_prompt = generate_prompt(data_point)
        return tokenize(full_prompt, tokenizer)
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å¤‰æ›ï¼ˆä¸¦åˆ—å‡¦ç†ã§é«˜é€ŸåŒ–ï¼‰
    train_data = data["train"].map(
        generate_and_tokenize_prompt,
        num_proc=4,  # ä¸¦åˆ—å‡¦ç†
        remove_columns=data["train"].column_names
    )
    
    return train_data

def prepare_model():
    """ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™ - A100æœ€é©åŒ–ç‰ˆ"""
    print("\n=== A100æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™ ===")
    
    # A100å‘ã‘æœ€é©åŒ–ã•ã‚ŒãŸé‡å­åŒ–è¨­å®š
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,  # 4bité‡å­åŒ–ã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡å‘ä¸Š
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )
    
    # ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        device_map="auto",
        torch_dtype=torch.float16,
    )
    
    # A100å‘ã‘æœ€é©åŒ–ã•ã‚ŒãŸLoRAè¨­å®š
    peft_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=8,  # å®‰å®šã—ãŸå€¤ã«æˆ»ã™
        lora_alpha=32,
        lora_dropout=0.1,
        target_modules=["query_key_value"]  # åŸºæœ¬çš„ãªã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«æˆ»ã™
    )
    
    # PEFTãƒ¢ãƒ‡ãƒ«ã®é©ç”¨
    model = get_peft_model(model, peft_config)
    
    # å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç¢ºèª
    model.print_trainable_parameters()
    
    return model

def train_model(model, tokenizer, train_data):
    """A100æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’"""
    print("\n=== A100æœ€é©åŒ–å­¦ç¿’ ===")
    
    # A100å‘ã‘æœ€é©åŒ–ã•ã‚ŒãŸå­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    training_args = TrainingArguments(
        output_dir=output_dir,
        overwrite_output_dir=True,
        num_train_epochs=1,
        per_device_train_batch_size=16,  # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’4å€ã«å¢—åŠ 
        gradient_accumulation_steps=2,   # å‹¾é…è“„ç©ã§ã•ã‚‰ã«åŠ¹æœçš„ãªãƒãƒƒãƒã‚µã‚¤ã‚ºã«
        warmup_steps=100,
        logging_steps=10,  # ãƒ­ã‚°é »åº¦ã‚’ä¸Šã’ã‚‹
        save_steps=500,
        eval_steps=500,
        save_total_limit=3,
        learning_rate=2e-4,  # å­¦ç¿’ç‡ã‚’ä¸Šã’ã‚‹
        fp16=True,
        dataloader_num_workers=4,  # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä¸¦åˆ—åŒ–
        remove_unused_columns=False,
        report_to="none",
        gradient_checkpointing=False,  # å‹¾é…ã‚¨ãƒ©ãƒ¼å›é¿ã®ãŸã‚ç„¡åŠ¹åŒ–
        optim="adamw_torch",  # å®‰å®šã—ãŸã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
    )
    
    # ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼ã®æº–å‚™
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
    )
    
    # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®æº–å‚™
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_data,
        data_collator=data_collator,
    )
    
    # å­¦ç¿’å®Ÿè¡Œ
    print("ğŸš€ A100æœ€é©åŒ–å­¦ç¿’é–‹å§‹...")
    model.config.use_cache = False
    trainer.train()
    model.config.use_cache = True
    
    # LoRAãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
    trainer.model.save_pretrained(peft_name)
    
    print(f"âœ… A100æœ€é©åŒ–LoRAãƒ¢ãƒ‡ãƒ«ã‚’ {peft_name} ã«ä¿å­˜ã—ã¾ã—ãŸ")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ğŸš€ Rinna-3.6B LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° - A100æœ€é©åŒ–ç‰ˆ")
    print("å‚è€ƒ: https://note.com/npaka/n/nc387b639e50e")
    print("=" * 60)
    
    # GPUæƒ…å ±ã®è¡¨ç¤º
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    
    # ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    setup_environment()
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®æº–å‚™
    tokenizer = prepare_tokenizer()
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™
    train_data = prepare_dataset(tokenizer)
    
    # ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™
    model = prepare_model()
    
    # å­¦ç¿’ã®å®Ÿè¡Œ
    train_model(model, tokenizer, train_data)
    
    print("\nğŸ‰ A100æœ€é©åŒ–å­¦ç¿’å®Œäº† ğŸ‰")
    print(f"LoRAãƒ¢ãƒ‡ãƒ«: {peft_name}")
    print(f"å­¦ç¿’çµæœ: {output_dir}")

if __name__ == "__main__":
    main() 